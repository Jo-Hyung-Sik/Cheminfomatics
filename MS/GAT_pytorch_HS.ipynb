{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JGUPvJbeX1v-"
      },
      "source": [
        "#Install RDkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc9mCz9rX00v"
      },
      "outputs": [],
      "source": [
        "#!wget https://github.com/Jo-Hyung-Sik/GCN-HS/tree/master/utils.py -O utils.py\n",
        "!mkdir results\n",
        "!mkdir images\n",
        "\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "# !time conda install -q -y -c conda-forge rdkit\n",
        "!apt-get install -y python3-rdkit librdkit1 rdkit-data\n",
        "!pip3 install rdkit\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import subprocess\n",
        "import shutil\n",
        "from logging import getLogger, StreamHandler, INFO\n",
        "\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "logger.addHandler(StreamHandler())\n",
        "logger.setLevel(INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQVCabklZXde"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import SimilarityMaps, rdMolDraw2D\n",
        "from rdkit.Chem import AllChem, rdDepictor, Draw, MolFromSmiles, QED, rdMolDescriptors, MolSurf\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "from numpy.polynomial.polynomial import polyfit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import argparse\n",
        "\n",
        "sys.setrecursionlimit(50000)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "torch.nn.Module.dump_patches = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2kXUWX5UPyD"
      },
      "source": [
        "# early stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppJIkAt3USGA",
        "outputId": "cc0e74ea-aa3c-4b9b-c0f9-7c1819def220"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    def save_checkpoint(self, val_loss, model):\\n        Saves model when validation loss decrease.\\n        if self.verbose:\\n            #print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\\n        torch.save(model.state_dict(), 'checkpoint.pt')\\n        self.val_loss_min = val_loss\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            #self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter == self.patience:\n",
        "                self.early_stop = True\n",
        "            elif self.counter > self.patience:\n",
        "                self.counter = 0\n",
        "                self.early_stop = False\n",
        "                self.best_score = None\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            #self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "'''\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        Saves model when validation loss decrease.\n",
        "        if self.verbose:\n",
        "            #print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WIAPzGs3UVF5"
      },
      "source": [
        "#데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fOYN62jeZkX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MolFromSmiles, Draw, rdDepictor, MolSurf\n",
        "from rdkit.Chem.Draw import rdMolDraw2D, SimilarityMaps\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.utils.data as Data\n",
        "\n",
        "import gc\n",
        "import sys\n",
        "import pickle\n",
        "torch.manual_seed(950228) # for reproduce\n",
        "sys.setrecursionlimit(50000)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "#from tensorboardX import SummaryWriter\n",
        "torch.nn.Module.dump_patches = True\n",
        "import copy\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from numpy.polynomial.polynomial import polyfit\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6np8cfUVKXzj",
        "outputId": "6ec5e710-2acd-48bb-ed98-c9519a563ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1sFfPRRUW8Z"
      },
      "outputs": [],
      "source": [
        "patience = 500\n",
        "\n",
        "# result file path\n",
        "result_file = \"/content/drive/MyDrive/data/GAT-최적화-2layer-0.0002-0.0004.csv\"\n",
        "\n",
        "early_stopping = EarlyStopping(patience, verbose=True)\n",
        "\n",
        "def get_splitted_dataset(ratios=[0.8, 0.2], seed=950228):\n",
        "    train_data = pd.read_csv('/content/drive/MyDrive/data/colab용/처리끝/train-smi(opt)-knime.csv') # Open original dataset\n",
        "    test_data = pd.read_csv('/content/drive/MyDrive/data/colab용/처리끝/test-smi(opt)-knime.csv') # Open original dataset\n",
        "\n",
        "    #smiles = train_data['smiles']\n",
        "\n",
        "    train, val = train_test_split(train_data, test_size=ratios[1], random_state=seed)\n",
        "\n",
        "    test = test_data\n",
        "\n",
        "    train, val, test = train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)\n",
        "\n",
        "    pd_save_column = pd.DataFrame(columns=['train_R2', 'train_MAE', 'train_MSE', 'val_R2', 'val_MAE', 'val_MSE', 'test_R2', 'test_MAE', 'test_MSE', 'n_layer', 'learning_rate', 'batch_size', 'out_dim', 'molvec_dim'])\n",
        "    pd_save_column.to_csv(result_file)\n",
        "\n",
        "    return train, val, test\n",
        "\n",
        "datasets = get_splitted_dataset()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eSpD5ZGrYyVN"
      },
      "source": [
        "#Featurizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj8nayqBYyHu"
      },
      "outputs": [],
      "source": [
        "LIST_SYMBOLS = ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
        "            'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
        "            'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
        "            'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']\n",
        "\n",
        "degrees = [0,1,2,3,4,5]\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(\n",
        "            x, allowable_set))\n",
        "    return [int(x == s) for s in allowable_set]\n",
        "\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return [int(x == s) for s in allowable_set]\n",
        "\n",
        "def atom_feature(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(), LIST_SYMBOLS) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6]) +\n",
        "                    one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
        "                    one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
        "                    one_of_k_encoding(int(atom.GetIsAromatic()), [0, 1]))    # (40, 7, 5, 6, 2)\n",
        "\n",
        "def bond_features(bond, use_chirality=True):\n",
        "    bt = bond.GetBondType()\n",
        "    bond_feats = [\n",
        "        int(bt == Chem.rdchem.BondType.SINGLE), int(bt == Chem.rdchem.BondType.DOUBLE),\n",
        "        int(bt == Chem.rdchem.BondType.TRIPLE), int(bt == Chem.rdchem.BondType.AROMATIC),\n",
        "        int(bond.GetIsConjugated()),\n",
        "        int(bond.IsInRing())\n",
        "    ]\n",
        "    if use_chirality:\n",
        "        bond_feats = bond_feats + one_of_k_encoding_unk(\n",
        "            str(bond.GetStereo()),\n",
        "            [\"STEREONONE\", \"STEREOANY\", \"STEREOZ\", \"STEREOE\"])\n",
        "    return np.array(bond_feats)\n",
        "\n",
        "def mol2graph(smi):\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    atom_size = 150\n",
        "    bond_size = 150\n",
        "    atom_feature_array = np.zeros((atom_size, 60), dtype=np.uint8)\n",
        "    bond_feature_array = np.empty((bond_size,10), dtype=np.uint8)\n",
        "    atom_idx = {}\n",
        "    mask = np.zeros((atom_size+1)) # +1 zero padding\n",
        "    degree_list = {}\n",
        "    atom_neighbor_list = []\n",
        "    bond_neighbor_list = []\n",
        "\n",
        "    for i, atom in enumerate(mol.GetAtoms()):\n",
        "        feature = atom_feature(atom)\n",
        "        atom_feature_array[i,:] = feature\n",
        "        atom_idx[atom.GetIdx()] = atom\n",
        "        mask[i] = 1.0\n",
        "        degree_list[atom.GetIdx()] = atom.GetDegree()\n",
        "        atom_neighbor_list.append(i)\n",
        "    # zero padding\n",
        "    atom_feature_array = np.append(atom_feature_array, np.zeros((1,60), dtype=np.uint8), axis=0)\n",
        "\n",
        "\n",
        "    for i, bond in enumerate(mol.GetBonds()):\n",
        "        atom1 = bond.GetBeginAtom().GetIdx()\n",
        "        atom2 = bond.GetEndAtom().GetIdx()\n",
        "\n",
        "        feature = bond_features(bond)\n",
        "        bond_feature_array[i,:] = feature\n",
        "\n",
        "        bond_neighbor_list.append([atom1, atom2])\n",
        "    # zero padding\n",
        "    bond_feature_array = np.append(bond_feature_array, np.zeros((1,10), dtype=np.uint8), axis=0)\n",
        "\n",
        "    # neighbors\n",
        "    atom_neighbors = np.zeros(((atom_size + 1), len(degrees)))\n",
        "    bond_neighbors = np.zeros(((bond_size + 1), len(degrees)))\n",
        "\n",
        "    atom_neighbors.fill(atom_size)\n",
        "    bond_neighbors.fill(bond_size)\n",
        "\n",
        "    all_bond_list = {}\n",
        "    for i, atom_neighbor in enumerate(atom_neighbor_list):\n",
        "        atom_neighbor_cnt = 0\n",
        "        bond_list = []\n",
        "        for j, bond_neighbor in enumerate(bond_neighbor_list):\n",
        "            if atom_neighbor in bond_neighbor:\n",
        "                remain_atom = bond_neighbor.copy()\n",
        "                remain_atom.remove(atom_neighbor)\n",
        "                atom_neighbors[i, atom_neighbor_cnt] = remain_atom[0]\n",
        "                atom_neighbor_cnt += 1\n",
        "                bond_list.append(j)\n",
        "\n",
        "        # all_bond_list[atom_neighbor_cnt] = [bond_list]\n",
        "        if atom_neighbor_cnt not in all_bond_list :\n",
        "            # print(bond_list)\n",
        "            all_bond_list[atom_neighbor_cnt] = [bond_list]\n",
        "        else:\n",
        "            all_bond_list[atom_neighbor_cnt].append(bond_list)\n",
        "\n",
        "    all_bond_list = dict(sorted(all_bond_list.items()))\n",
        "    bond_neighbor_cnt = 0\n",
        "    for key, values in all_bond_list.items():\n",
        "        for value in values:\n",
        "            for i, val in enumerate(value):\n",
        "                bond_neighbors[bond_neighbor_cnt, i] = val\n",
        "            bond_neighbor_cnt += 1\n",
        "\n",
        "\n",
        "    return {'mask': mask, 'atom_feature': atom_feature_array, 'bond_feature': bond_feature_array, 'atom_neighbor': atom_neighbors, 'bond_neighbor': bond_neighbors}\n",
        "\n",
        "def get_smiles_array(smilesList):\n",
        "    x_mask = []\n",
        "    x_atom = []\n",
        "    x_bonds = []\n",
        "    x_atom_index = []\n",
        "    x_bond_index = []\n",
        "    for smiles in smilesList:\n",
        "        feature_dicts = mol2graph(smiles)\n",
        "        x_mask.append(feature_dicts['mask'])\n",
        "        x_atom.append(feature_dicts['atom_feature'])\n",
        "        x_bonds.append(feature_dicts['bond_feature'])\n",
        "        x_atom_index.append(feature_dicts['atom_neighbor'])\n",
        "        x_bond_index.append(feature_dicts['bond_neighbor'])\n",
        "    return np.asarray(x_atom),np.asarray(x_bonds),np.asarray(x_atom_index),np.asarray(x_bond_index),np.asarray(x_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sphwkPiVXRl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class gatDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.smiles = df['smiles']\n",
        "        self.exp = df['exp'].values\n",
        "\n",
        "        list_mask = list()\n",
        "        list_atom_feature = list()\n",
        "        list_bond_feature = list()\n",
        "        list_atom_neighbor = list()\n",
        "        list_bond_neighbor = list()\n",
        "        for i, smiles in enumerate(self.smiles):\n",
        "            result_dict = mol2graph(smiles)\n",
        "\n",
        "            list_mask.append(result_dict['mask'])\n",
        "            list_atom_feature.append(result_dict['atom_feature'])\n",
        "            list_bond_feature.append(result_dict['bond_feature'])\n",
        "            list_atom_neighbor.append(result_dict['atom_neighbor'])\n",
        "            list_bond_neighbor.append(result_dict['bond_neighbor'])\n",
        "\n",
        "        self.mask = np.asarray(list_mask)\n",
        "        self.list_atom_feature = np.asarray(list_atom_feature)\n",
        "        self.list_bond_feature = np.asarray(list_bond_feature)\n",
        "        self.list_atom_neighbor = np.asarray(list_atom_neighbor)\n",
        "        self.list_bond_neighbor = np.asarray(list_bond_neighbor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_atom_feature)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.mask[index], self.list_atom_feature[index], self.list_bond_feature[index], self.list_atom_neighbor[index], self.list_bond_neighbor[index], self.exp[index]\n",
        "\n",
        "sample_dataset = gatDataset(datasets[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VDmmsGz3Xk6d"
      },
      "source": [
        "ㅊ#Attentive layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jRhTux91cV6"
      },
      "outputs": [],
      "source": [
        "class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.fc = nn.Linear(args.in_dim, args.out_dim)\n",
        "        self.neighbor_fc = nn.Linear(args.in_dim+args.in_bond_dim, args.out_dim)\n",
        "        self.GRUCell = nn.GRUCell(args.out_dim, args.out_dim)\n",
        "        self.align = nn.Linear(2*args.out_dim,1)\n",
        "        self.attend = nn.Linear(args.out_dim, args.out_dim)\n",
        "\n",
        "        self.mol_GRUCell = nn.GRUCell(args.out_dim, args.out_dim)\n",
        "        self.mol_align = nn.Linear(2*args.out_dim,1)\n",
        "        self.mol_attend = nn.Linear(args.out_dim, args.out_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "        self.output = nn.Linear(args.out_dim, args.output_unit)\n",
        "\n",
        "        self.radius = args.radius\n",
        "        self.T = args.T\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "    def forward(self, atom_list, bond_list, atom_degree_list, bond_degree_list, atom_mask):\n",
        "        atom_mask = atom_mask.unsqueeze(2)\n",
        "        batch, mol_length, num_atom_feat = atom_list.size()\n",
        "        atom_feature = F.leaky_relu(self.fc(atom_list))\n",
        "\n",
        "        bond_neighbor = [bond_list[i][bond_degree_list[i]] for i in range(batch)]\n",
        "        atom_neighbor = [atom_list[i][atom_degree_list[i]] for i in range(batch)]\n",
        "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
        "        atom_neighbor = torch.stack(atom_neighbor, dim=0)\n",
        "        neighbor_feature = torch.cat([atom_neighbor, bond_neighbor],dim=-1)\n",
        "        neighbor_feature = F.leaky_relu(self.neighbor_fc(neighbor_feature))\n",
        "\n",
        "        # generate mask to eliminate the influence of blank atoms\n",
        "        attend_mask = atom_degree_list.clone()\n",
        "        attend_mask[attend_mask != mol_length-1] = 1\n",
        "        attend_mask[attend_mask == mol_length-1] = 0\n",
        "        # attend_mask = attend_mask.type(torch.cuda.FloatTensor).unsqueeze(-1)\n",
        "        attend_mask = attend_mask.type(torch.FloatTensor).unsqueeze(-1)\n",
        "\n",
        "        softmax_mask = atom_degree_list.clone()\n",
        "        softmax_mask[softmax_mask != mol_length-1] = 0\n",
        "        softmax_mask[softmax_mask == mol_length-1] = -9e8\n",
        "        # softmax_mask = softmax_mask.type(torch.cuda.FloatTensor).unsqueeze(-1)\n",
        "        softmax_mask = softmax_mask.type(torch.FloatTensor).unsqueeze(-1)\n",
        "\n",
        "        batch, mol_length, max_neighbor_num, fingerprint_dim = neighbor_feature.shape\n",
        "        atom_feature_expand = atom_feature.unsqueeze(-2).expand(batch, mol_length, max_neighbor_num, fingerprint_dim)\n",
        "        feature_align = torch.cat([atom_feature_expand, neighbor_feature],dim=-1)\n",
        "        align_score = F.leaky_relu(self.align(self.dropout(feature_align))) + softmax_mask\n",
        "\n",
        "        attention_weight = F.softmax(align_score,-2) * attend_mask\n",
        "        neighbor_feature_transform = self.attend(self.dropout(neighbor_feature))\n",
        "\n",
        "        context = torch.sum(torch.mul(attention_weight,neighbor_feature_transform),-2)\n",
        "        context = F.elu(context)\n",
        "        context_reshape = context.view(batch*mol_length, fingerprint_dim)\n",
        "\n",
        "        atom_feature_reshape = atom_feature.view(batch*mol_length, fingerprint_dim)\n",
        "        atom_feature_reshape = self.GRUCell(context_reshape, atom_feature_reshape) # readout\n",
        "        atom_feature = atom_feature_reshape.view(batch, mol_length, fingerprint_dim)\n",
        "\n",
        "        #do nonlinearity\n",
        "        activated_features = F.relu(atom_feature)\n",
        "\n",
        "        # neigbhor atom attention in radius\n",
        "        for d in range(self.radius-1):\n",
        "            neighbor_feature = [activated_features[i][atom_degree_list[i]] for i in range(self.batch_size)]\n",
        "            neighbor_feature = torch.stack(neighbor_feature, dim=0)\n",
        "            atom_feature_expand = activated_features.unsqueeze(-2).expand(self.batch_size, mol_length, max_neighbor_num, fingerprint_dim)\n",
        "\n",
        "            feature_align = torch.cat([atom_feature_expand, neighbor_feature],dim=-1)\n",
        "            align_score = F.leaky_relu(self.align(self.dropout(feature_align))) + softmax_mask\n",
        "\n",
        "            attention_weight = F.softmax(align_score,-2) * attend_mask\n",
        "            neighbor_feature_transform = self.attend(self.dropout(neighbor_feature))\n",
        "\n",
        "            context = torch.sum(torch.mul(attention_weight,neighbor_feature_transform),-2)\n",
        "            context = F.elu(context)\n",
        "            context_reshape = context.view(batch*mol_length, fingerprint_dim)\n",
        "\n",
        "            atom_feature_reshape = self.GRUCell(context_reshape, atom_feature_reshape) # readout\n",
        "            atom_feature = atom_feature_reshape.view(batch, mol_length, fingerprint_dim)\n",
        "\n",
        "            activated_features = F.relu(atom_feature)\n",
        "\n",
        "        mol_feature = torch.sum(activated_features * atom_mask, dim=-2)\n",
        "        activated_features_mol = F.relu(mol_feature)\n",
        "\n",
        "        mol_softmax_mask = atom_mask.clone()\n",
        "        mol_softmax_mask[mol_softmax_mask == 0] = -9e8\n",
        "        mol_softmax_mask[mol_softmax_mask == 1] = 0\n",
        "        # mol_softmax_mask = mol_softmax_mask.type(torch.cuda.FloatTensor)\n",
        "        mol_softmax_mask = mol_softmax_mask.type(torch.FloatTensor)\n",
        "\n",
        "        for t in range(self.T):\n",
        "            mol_prediction_expand = activated_features_mol.unsqueeze(-2).expand(batch, mol_length, fingerprint_dim)\n",
        "\n",
        "            mol_align = torch.cat([mol_prediction_expand, activated_features], dim=-1)\n",
        "            mol_align_score = F.leaky_relu(self.mol_align(mol_align)) + mol_softmax_mask\n",
        "\n",
        "            mol_attention_weight = F.softmax(mol_align_score,-2) * atom_mask\n",
        "            activated_features_transform = self.mol_attend(self.dropout(activated_features))\n",
        "\n",
        "            mol_context = torch.sum(torch.mul(mol_attention_weight,activated_features_transform),-2)\n",
        "            mol_context = F.elu(mol_context)\n",
        "            mol_feature = self.mol_GRUCell(mol_context, mol_feature)\n",
        "\n",
        "            # do nonlinearity\n",
        "            activated_features_mol = F.relu(mol_feature)\n",
        "\n",
        "        mol_prediction = self.output(self.dropout(mol_feature))\n",
        "\n",
        "        return atom_feature, mol_prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I_dktWAPOCRB"
      },
      "source": [
        "# Train, Validation, Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp5HaufKOFR0"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, args, **kwargs):\n",
        "    epoch_train_loss = 0\n",
        "    list_train_loss = list()\n",
        "    tr_list_y, tr_list_pred_y = list(), list()\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        x_mask, x_atom, x_bond, x_atom_index, x_bond_index, y = batch[0].float(), batch[1].float(), batch[2].float(), batch[3].long(), batch[4].long(), batch[5].float()\n",
        "        x_mask, x_atom, x_bond, x_atom_index, x_bond_index, y = x_mask.to(args.device), x_atom.to(args.device), x_bond.to(args.device), x_atom_index.to(args.device), x_bond_index.to(args.device), y.to(args.device)\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        atom_prediction, mol_prediction = model(x_atom, x_bond, x_atom_index, x_bond_index, x_mask)\n",
        "\n",
        "        train_loss = criterion(mol_prediction, y)\n",
        "        epoch_train_loss += train_loss.item()\n",
        "        list_train_loss.append({'epoch':batch_idx/len(dataloader)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        tr_list_y += y.cpu().detach().numpy().tolist()\n",
        "        tr_list_pred_y += mol_prediction.cpu().detach().numpy().tolist()\n",
        "\n",
        "    tr_r2 = r2_score(tr_list_y, tr_list_pred_y)\n",
        "    tr_mae = mean_absolute_error(tr_list_y, tr_list_pred_y)\n",
        "    tr_mse = mean_squared_error(tr_list_y, tr_list_pred_y)\n",
        "\n",
        "    return model, list_train_loss, tr_r2, tr_mae, tr_mse, train_loss\n",
        "\n",
        "def eval(model, dataloader, criterion, args):\n",
        "    epoch_eval_loss = 0\n",
        "    cnt_iter = 0\n",
        "    eval_list_y, eval_list_pred_y = list(), list()\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        x_mask, x_atom, x_bond, x_atom_index, x_bond_index, y = batch[0].float(), batch[1].float(), batch[2].float(), batch[3].long(), batch[4].long(), batch[5].float()\n",
        "        x_mask, x_atom, x_bond, x_atom_index, x_bond_index, y = x_mask.to(args.device), x_atom.to(args.device), x_bond.to(args.device), x_atom_index.to(args.device), x_bond_index.to(args.device), y.to(args.device)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        atom_prediction, mol_prediction = model(x_atom, x_bond, x_atom_index, x_bond_index, x_mask)\n",
        "\n",
        "        eval_loss = criterion(mol_prediction, y)\n",
        "        epoch_eval_loss += eval_loss.item()\n",
        "        cnt_iter += 1\n",
        "        eval_list_y += y.cpu().detach().numpy().tolist()\n",
        "        eval_list_pred_y += mol_prediction.cpu().detach().numpy().tolist()\n",
        "\n",
        "    eval_r2 = r2_score(eval_list_y, eval_list_pred_y)\n",
        "    eval_mae = mean_absolute_error(eval_list_y, eval_list_pred_y)\n",
        "    eval_mse = mean_squared_error(eval_list_y, eval_list_pred_y)\n",
        "\n",
        "    return epoch_val_loss/cnt_iter , val_r2, val_mae, val_mse\n",
        "\n",
        "def experiment(partition, args):\n",
        "    ts = time.time()\n",
        "    tf_writer = SummaryWriter()\n",
        "\n",
        "    model = GAT(args)\n",
        "    model.to(args.device)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
        "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "    # Train, Validate, Evaluate\n",
        "    list_train_loss = list()\n",
        "    list_val_loss = list()\n",
        "    list_mae = list()\n",
        "    list_std = list()\n",
        "    list_mse = list()\n",
        "    list_r2 = list()\n",
        "    list_tr_r2 = list()\n",
        "    list_tr_mae = list()\n",
        "    list_tr_mse = list()\n",
        "    list_val_r2 = list()\n",
        "    list_val_mae = list()\n",
        "    list_val_mse = list()\n",
        "\n",
        "    # save parameter\n",
        "    save_mae = list()\n",
        "    save_mse = list()\n",
        "    save_r2 = list()\n",
        "    save_tr_r2 = list()\n",
        "    save_tr_mae = list()\n",
        "    save_tr_mse = list()\n",
        "    save_val_r2 = list()\n",
        "    save_val_mae = list()\n",
        "    save_val_mse = list()\n",
        "    save_result = list()\n",
        "    save_epoch = list()\n",
        "\n",
        "    args.best_val_loss = 10000\n",
        "    for epoch in range(args.epoch):\n",
        "        model, train_losses , tr_r2, tr_mae, tr_mse, tr_loss= train(model, partition['train'], optimizer, criterion, args, **{'epoch':epoch})\n",
        "        val_loss, val_r2, val_mae, val_mse= eval(model, partition['val'], criterion, args)\n",
        "        ts_r2, ts_mae, ts_mse, std, true_y, pred_y = eval(model, partition['test'], args, **{'epoch':epoch})\n",
        "\n",
        "        list_train_loss += train_losses\n",
        "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
        "        list_r2.append({'epoch':epoch, 'r2':ts_r2})\n",
        "        list_mae.append({'epoch':epoch, 'mae':ts_mae})\n",
        "        list_mse.append({'epoch':epoch, 'mse':ts_mse})\n",
        "        list_std.append({'epoch':epoch, 'std':std})\n",
        "        list_tr_r2.append({'epoch':epoch, 'r2':tr_r2})\n",
        "        list_tr_mae.append({'epoch':epoch, 'mae':tr_mae})\n",
        "        list_tr_mse.append({'epoch':epoch, 'mse':tr_mse})\n",
        "        list_val_r2.append({'epoch':epoch, 'r2':val_r2})\n",
        "        list_val_mae.append({'epoch':epoch, 'mae':val_mae})\n",
        "        list_val_mse.append({'epoch':epoch, 'mse':val_mse})\n",
        "\n",
        "        save_tr_r2.append(tr_r2)\n",
        "        save_tr_mae.append(tr_mae)\n",
        "        save_tr_mse.append(tr_mse)\n",
        "        save_val_r2.append(val_r2)\n",
        "        save_val_mae.append(val_mae)\n",
        "        save_val_mse.append(val_mse)\n",
        "        save_r2.append(ts_r2)\n",
        "        save_mae.append(ts_mae)\n",
        "        save_mse.append(ts_mse)\n",
        "        save_epoch.append(epoch)\n",
        "\n",
        "        tf_writer.add_scalar('Loss/train', tr_loss, epoch)\n",
        "        tf_writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "        tf_writer.add_scalar('MSE/test', ts_mae, epoch)\n",
        "\n",
        "        if args.best_val_loss > val_loss or epoch==0:\n",
        "            args.best_val_loss\n",
        "            args.best_epoch = epoch\n",
        "            args.best_r2 = ts_r2\n",
        "            args.best_mae = ts_mae\n",
        "            args.best_mse = ts_mse\n",
        "            args.best_std = std\n",
        "            args.best_true_y = true_y\n",
        "            args.best_pred_y = pred_y\n",
        "            args.best_tr_r2 = tr_r2\n",
        "            args.best_tr_mae = tr_mae\n",
        "            args.best_tr_mse = tr_mse\n",
        "            args.best_val_r2 = val_r2\n",
        "            args.best_val_mae = val_mae\n",
        "            args.best_val_mse = val_mse\n",
        "            '''\n",
        "            tf_writer.add_hparams(\n",
        "                hparam_dict = vars(args),\n",
        "                metric_dict={'best_ts_mse' : args.best_mse, 'best_epoch':args.best_epoch}\n",
        "            )\n",
        "            '''\n",
        "\n",
        "        early_stopping(val_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # save result .csv\n",
        "    for save_iter in range(len(list_val_mse)):\n",
        "        save_list = {'val_MSE' : save_val_mse[save_iter], 'val_MAE' : save_val_mae[save_iter], 'val_R2' : save_val_r2[save_iter], 'train_MSE' : save_tr_mse[save_iter], 'train_MAE' : save_tr_mae[save_iter], 'train_R2' : save_tr_r2[save_iter], 'test_MSE' : save_mse[save_iter], 'test_MAE' : save_mae[save_iter], 'test_R2' : save_r2[save_iter], 'epoch' : save_epoch[save_iter], 'n_layer' : args.n_layer, 'learning_rate' : args.lr, 'batch_size' : args.batch_size, 'out_dim' : args.out_dim, 'molvec_dim' : args.molvec_dim}\n",
        "        save_result.append(save_list)\n",
        "    pd_save = pd.DataFrame(save_result, columns=['train_R2', 'train_MAE', 'train_MSE', 'val_R2', 'val_MAE', 'val_MSE', 'test_R2', 'test_MAE', 'test_MSE', 'n_layer', 'learning_rate', 'batch_size', 'out_dim', 'molvec_dim'])\n",
        "    pd_save.to_csv(result_file , mode='a', header=False)\n",
        "\n",
        "    # End of experiments\n",
        "    te = time.time()\n",
        "    args.elapsed = te-ts\n",
        "    args.train_losses = list_train_loss\n",
        "    args.val_losses = list_val_loss\n",
        "    args.maes = list_mae\n",
        "    args.stds = list_std\n",
        "    return model, args"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vFCoT1H5X5bm"
      },
      "source": [
        "#Model running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CXPoyKLYEHf"
      },
      "outputs": [],
      "source": [
        "seed = 950228\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# ==== Training Config ==== #\n",
        "args.batch_size = 128\n",
        "args.epoch = 10\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ==== Model Architercutre ==== #\n",
        "args.in_dim = 60\n",
        "args.in_bond_dim = 10\n",
        "args.dropout= 0.15\n",
        "args.out_dim = 256\n",
        "\n",
        "# ==== Optimizer Config ==== #\n",
        "args.weight_decay_value = 0.00001\n",
        "args.lr = 0.001\n",
        "args.l2_coef = 0.0001\n",
        "args.radius = 3\n",
        "args.T = 1\n",
        "args.per_task_output_units_num = 1 # for regression model\n",
        "args.output_unit = 1\n",
        "\n",
        "# writer = Writer(prior_keyword=['n_layer', 'use_bn', 'lr', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
        "# writer.clear()\n",
        "\n",
        "# Define Hyperparameter Search Space\n",
        "list_lr = [0.0001, 0.0005, 0.001]\n",
        "list_batch_size = [128]\n",
        "\n",
        "# partition = {'train': datasets[0], 'val': datasets[1], 'test': datasets[2]}\n",
        "train_dataloader = DataLoader(gatDataset(datasets[0]), batch_size=args.batch_size, shuffle=True, generator=torch.Generator(device=args.device))\n",
        "val_dataloader = DataLoader(gatDataset(datasets[1]), batch_size=args.batch_size, shuffle=False, generator=torch.Generator(device=args.device))\n",
        "test_dataloader = DataLoader(gatDataset(datasets[2]), batch_size=args.batch_size, shuffle=False, generator=torch.Generator(device=args.device))\n",
        "partition = {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
        "\n",
        "for lr in list_lr:\n",
        "    args.lr = lr\n",
        "\n",
        "    model, result = experiment(partition, args)\n",
        "    # writer.write(result)\n",
        "\n",
        "    print('[Exp {:2}] training tr_r2: {:2.3f}, tr_mae: {:2.3f}, tr_mse: {:2.3f} validation val_r2: {:2.3f}, val_mae: {:2.3f}, val_mse: {:2.3f} test ts_r2: {:2.3f}, ts_mae: {:2.3f}, ts_mse: {:2.3f}, std: {:2.3f} at epoch {:2} took {:3.1f} sec'.format(cnt_exp, result.best_tr_r2, result.best_tr_mae, result.best_tr_mse, result.best_val_r2, result.best_val_mae, result.best_val_mse, result.best_r2, result.best_mae, result.best_mse, result.best_std, result.best_epoch, result.elapsed))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y2kXUWX5UPyD"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
